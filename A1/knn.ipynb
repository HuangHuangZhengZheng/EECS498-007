{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QcJK3kXl--c3"
   },
   "source": [
    "# EECS 498-007/598-005 Assignment 1-2: K-Nearest Neighbors (k-NN)\n",
    "\n",
    "Before we start, please put your name and UMID in following format\n",
    "\n",
    ": Firstname LASTNAME, #00000000   //   e.g.) Justin JOHNSON, #12345678"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7sA2iBcm_cPb"
   },
   "source": [
    "**Your Answer:**   \n",
    "Your NAME, #XXXXXXXX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qc83ETI1a3o9"
   },
   "source": [
    "In this notebook you will implement a K-Nearest Neighbors classifier on the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).\n",
    "\n",
    "Recall that the K-Nearest Neighbor classifier does the following:\n",
    "- During training, the classifier simply memorizes the training data\n",
    "- During testing, test images are compared to each training image; the predicted label is the majority vote among the K nearest training examples.\n",
    "\n",
    "After implementing the K-Nearest Neighbor classifier, you will use *cross-validation* to find the best value of K.\n",
    "\n",
    "The goals of this exercise are to go through a simple example of the data-driven image classification pipeline, and also to practice writing efficient, vectorized code in [PyTorch](https://pytorch.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hQrEwOpXb9Gh"
   },
   "source": [
    "# Setup Code\n",
    "Before getting started we need to run some boilerplate code to set up our environment. You'll need to rerun this setup code each time you start the notebook.\n",
    "\n",
    "First, run this cell load the [autoreload](https://ipython.readthedocs.io/en/stable/config/extensions/autoreload.html?highlight=autoreload) extension. This allows us to edit `.py` source files, and re-import them into the notebook for a seamless editing and debugging experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "73cuTs3re6wg"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cnf0BfHZfWzO"
   },
   "source": [
    "### Google Colab Setup\n",
    "Next we need to run a few commands to set up our environment on Google Colab. If you are running this notebook on a local machine you can skip this section.\n",
    "\n",
    "Run the following cell to mount your Google Drive. Follow the link, sign in to your Google account (the same account you used to store this notebook!) and copy the authorization code into the text box that appears below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "VxbQtNB6fWzO",
    "outputId": "036f84c0-47cb-48ee-a0e7-3a0a356e9a0e"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IW2eBtZsfWzR"
   },
   "source": [
    "Now recall the path in your Google Drive where you uploaded this notebook, fill it in below. If everything is working correctly then running the folowing cell should print the filenames from the assignment:\n",
    "\n",
    "```\n",
    "['pytorch101.py', 'knn.py', 'knn.ipynb', 'eecs598', 'pytorch101.ipynb']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "kfzFXbiEfWzS",
    "outputId": "3d963b4b-806e-42c0-e113-675c3fe44bf4"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# TODO: Fill in the Google Drive path where you uploaded the assignment\n",
    "# Example: If you create a 2020FA folder and put all the files under A1 folder, then '2020FA/A1'\n",
    "# GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = '2020FA/A1'\n",
    "GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = None\n",
    "GOOGLE_DRIVE_PATH = os.path.join('drive', 'My Drive', GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)\n",
    "print(os.listdir(GOOGLE_DRIVE_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aY_PV4eQfWzU"
   },
   "source": [
    "Once you have successfully mounted your Google Drive and located the path to this assignment, run th following cell to allow us to import from the `.py` files of this assignment. If it works correctly, it should print the message:\n",
    "\n",
    "```\n",
    "Hello from knn.py!\n",
    "```\n",
    "\n",
    "as well as the last edit time for the file `knn.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "VGbUf6nTfWzV",
    "outputId": "5c1abe99-b170-4f9b-f3fb-2dd3e7b015a3"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(GOOGLE_DRIVE_PATH)\n",
    "\n",
    "import time, os\n",
    "os.environ[\"TZ\"] = \"US/Eastern\"\n",
    "time.tzset()\n",
    "\n",
    "from knn import hello\n",
    "hello()\n",
    "\n",
    "knn_path = os.path.join(GOOGLE_DRIVE_PATH, 'knn.py')\n",
    "knn_edit_time = time.ctime(os.path.getmtime(knn_path))\n",
    "print('knn.py last edited on %s' % knn_edit_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SWSgBT8Wf3tW"
   },
   "source": [
    "# Data preprocessing / Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "emQnvtnFeX1H"
   },
   "source": [
    "## Setup code\n",
    "Run some setup code for this notebook: Import some useful packages and increase the default figure size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tf64a0TS8zh7"
   },
   "outputs": [],
   "source": [
    "import eecs598\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "plt.rcParams['font.size'] = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GSd6jQb4epkC"
   },
   "source": [
    "## Load the CIFAR-10 dataset\n",
    "The utility function `eecs598.data.cifar10()` returns the entire CIFAR-10 dataset as a set of four **Torch tensors**:\n",
    "\n",
    "- `x_train` contains all training images (real numbers in the range $[0, 1]$)\n",
    "- `y_train` contains all training labels (integers in the range $[0, 9]$)\n",
    "- `x_test` contains all test images\n",
    "- `y_test` contains all test labels\n",
    "\n",
    "This function automatically downloads the CIFAR-10 dataset the first time you run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "y2JiLb-R9bFb",
    "outputId": "b3ac93f5-5e8d-4152-aa04-28a482a4dcc4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to .\\cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170498071/170498071 [00:16<00:00, 10584381.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting .\\cifar-10-python.tar.gz to .\n",
      "Training set:\n",
      "  data shape: torch.Size([50000, 3, 32, 32])\n",
      "  labels shape:  torch.Size([50000])\n",
      "Test set:\n",
      "  data shape:  torch.Size([10000, 3, 32, 32])\n",
      "  labels shape torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_test, y_test = eecs598.data.cifar10()\n",
    "\n",
    "print('Training set:', )\n",
    "print('  data shape:', x_train.shape)\n",
    "print('  labels shape: ', y_train.shape)\n",
    "print('Test set:')\n",
    "print('  data shape: ', x_test.shape)\n",
    "print('  labels shape', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AKKdLGIIffYx"
   },
   "source": [
    "## Visualize the dataset\n",
    "To give you a sense of the nature of the images in CIFAR-10, this cell visualizes some random examples from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 483
    },
    "colab_type": "code",
    "id": "UMNVrzrd-d_y",
    "outputId": "1bd00712-3a50-4e4d-fb39-a9353c25c56e"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Numpy is not available",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m         samples\u001b[38;5;241m.\u001b[39mappend(x_train[idx])\n\u001b[0;32m     13\u001b[0m img \u001b[38;5;241m=\u001b[39m torchvision\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mmake_grid(samples, nrow\u001b[38;5;241m=\u001b[39msamples_per_class)\n\u001b[1;32m---> 14\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(\u001b[43meecs598\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor_to_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     15\u001b[0m plt\u001b[38;5;241m.\u001b[39maxis(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moff\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     16\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32mc:\\Users\\24349\\Desktop\\A1\\eecs598\\utils.py:33\u001b[0m, in \u001b[0;36mtensor_to_image\u001b[1;34m(tensor)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;124;03mConvert a torch tensor into a numpy ndarray for visualization.\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;124;03m- ndarr: A uint8 numpy array of shape (H, W, 3)\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     32\u001b[0m tensor \u001b[38;5;241m=\u001b[39m tensor\u001b[38;5;241m.\u001b[39mmul(\u001b[38;5;241m255\u001b[39m)\u001b[38;5;241m.\u001b[39madd_(\u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mclamp_(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255\u001b[39m)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 33\u001b[0m ndarr \u001b[38;5;241m=\u001b[39m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muint8\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ndarr\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Numpy is not available"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Image size of 3974x199649 pixels is too large. It must be less than 2^16 in each direction.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\formatters.py:343\u001b[0m, in \u001b[0;36mBaseFormatter.__call__\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    341\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    342\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprinter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# Finally look for special method names\u001b[39;00m\n\u001b[0;32m    345\u001b[0m method \u001b[38;5;241m=\u001b[39m get_real_method(obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_method)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\pylabtools.py:152\u001b[0m, in \u001b[0;36mprint_figure\u001b[1;34m(fig, fmt, bbox_inches, base64, **kwargs)\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend_bases\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FigureCanvasBase\n\u001b[0;32m    150\u001b[0m     FigureCanvasBase(fig)\n\u001b[1;32m--> 152\u001b[0m \u001b[43mfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcanvas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprint_figure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbytes_io\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    153\u001b[0m data \u001b[38;5;241m=\u001b[39m bytes_io\u001b[38;5;241m.\u001b[39mgetvalue()\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fmt \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msvg\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\24349\\.conda\\envs\\PRPsu24\\Lib\\site-packages\\matplotlib\\backend_bases.py:2204\u001b[0m, in \u001b[0;36mFigureCanvasBase.print_figure\u001b[1;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[0;32m   2200\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   2201\u001b[0m     \u001b[38;5;66;03m# _get_renderer may change the figure dpi (as vector formats\u001b[39;00m\n\u001b[0;32m   2202\u001b[0m     \u001b[38;5;66;03m# force the figure dpi to 72), so we need to set it again here.\u001b[39;00m\n\u001b[0;32m   2203\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m cbook\u001b[38;5;241m.\u001b[39m_setattr_cm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure, dpi\u001b[38;5;241m=\u001b[39mdpi):\n\u001b[1;32m-> 2204\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mprint_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2205\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2206\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfacecolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfacecolor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2207\u001b[0m \u001b[43m            \u001b[49m\u001b[43medgecolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medgecolor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2208\u001b[0m \u001b[43m            \u001b[49m\u001b[43morientation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morientation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2209\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbbox_inches_restore\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_bbox_inches_restore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2210\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2211\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   2212\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m bbox_inches \u001b[38;5;129;01mand\u001b[39;00m restore_bbox:\n",
      "File \u001b[1;32mc:\\Users\\24349\\.conda\\envs\\PRPsu24\\Lib\\site-packages\\matplotlib\\backend_bases.py:2054\u001b[0m, in \u001b[0;36mFigureCanvasBase._switch_canvas_and_return_print_method.<locals>.<lambda>\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   2050\u001b[0m     optional_kws \u001b[38;5;241m=\u001b[39m {  \u001b[38;5;66;03m# Passed by print_figure for other renderers.\u001b[39;00m\n\u001b[0;32m   2051\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdpi\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacecolor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124medgecolor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morientation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2052\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbbox_inches_restore\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m   2053\u001b[0m     skip \u001b[38;5;241m=\u001b[39m optional_kws \u001b[38;5;241m-\u001b[39m {\u001b[38;5;241m*\u001b[39minspect\u001b[38;5;241m.\u001b[39msignature(meth)\u001b[38;5;241m.\u001b[39mparameters}\n\u001b[1;32m-> 2054\u001b[0m     print_method \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mwraps(meth)(\u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2055\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mskip\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   2056\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# Let third-parties do as they see fit.\u001b[39;00m\n\u001b[0;32m   2057\u001b[0m     print_method \u001b[38;5;241m=\u001b[39m meth\n",
      "File \u001b[1;32mc:\\Users\\24349\\.conda\\envs\\PRPsu24\\Lib\\site-packages\\matplotlib\\backends\\backend_agg.py:496\u001b[0m, in \u001b[0;36mFigureCanvasAgg.print_png\u001b[1;34m(self, filename_or_obj, metadata, pil_kwargs)\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprint_png\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename_or_obj, \u001b[38;5;241m*\u001b[39m, metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, pil_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    450\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    451\u001b[0m \u001b[38;5;124;03m    Write the figure to a PNG file.\u001b[39;00m\n\u001b[0;32m    452\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    494\u001b[0m \u001b[38;5;124;03m        *metadata*, including the default 'Software' key.\u001b[39;00m\n\u001b[0;32m    495\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 496\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_print_pil\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpng\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpil_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\24349\\.conda\\envs\\PRPsu24\\Lib\\site-packages\\matplotlib\\backends\\backend_agg.py:444\u001b[0m, in \u001b[0;36mFigureCanvasAgg._print_pil\u001b[1;34m(self, filename_or_obj, fmt, pil_kwargs, metadata)\u001b[0m\n\u001b[0;32m    439\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_print_pil\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename_or_obj, fmt, pil_kwargs, metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    440\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    441\u001b[0m \u001b[38;5;124;03m    Draw the canvas, then save it using `.image.imsave` (to which\u001b[39;00m\n\u001b[0;32m    442\u001b[0m \u001b[38;5;124;03m    *pil_kwargs* and *metadata* are forwarded).\u001b[39;00m\n\u001b[0;32m    443\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 444\u001b[0m     \u001b[43mFigureCanvasAgg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    445\u001b[0m     mpl\u001b[38;5;241m.\u001b[39mimage\u001b[38;5;241m.\u001b[39mimsave(\n\u001b[0;32m    446\u001b[0m         filename_or_obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer_rgba(), \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39mfmt, origin\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupper\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    447\u001b[0m         dpi\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure\u001b[38;5;241m.\u001b[39mdpi, metadata\u001b[38;5;241m=\u001b[39mmetadata, pil_kwargs\u001b[38;5;241m=\u001b[39mpil_kwargs)\n",
      "File \u001b[1;32mc:\\Users\\24349\\.conda\\envs\\PRPsu24\\Lib\\site-packages\\matplotlib\\backends\\backend_agg.py:382\u001b[0m, in \u001b[0;36mFigureCanvasAgg.draw\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdraw\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    381\u001b[0m     \u001b[38;5;66;03m# docstring inherited\u001b[39;00m\n\u001b[1;32m--> 382\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrenderer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_renderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrenderer\u001b[38;5;241m.\u001b[39mclear()\n\u001b[0;32m    384\u001b[0m     \u001b[38;5;66;03m# Acquire a lock on the shared font cache.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\24349\\.conda\\envs\\PRPsu24\\Lib\\site-packages\\matplotlib\\backends\\backend_agg.py:397\u001b[0m, in \u001b[0;36mFigureCanvasAgg.get_renderer\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    395\u001b[0m reuse_renderer \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lastKey \u001b[38;5;241m==\u001b[39m key)\n\u001b[0;32m    396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m reuse_renderer:\n\u001b[1;32m--> 397\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrenderer \u001b[38;5;241m=\u001b[39m \u001b[43mRendererAgg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdpi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    398\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lastKey \u001b[38;5;241m=\u001b[39m key\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrenderer\n",
      "File \u001b[1;32mc:\\Users\\24349\\.conda\\envs\\PRPsu24\\Lib\\site-packages\\matplotlib\\backends\\backend_agg.py:70\u001b[0m, in \u001b[0;36mRendererAgg.__init__\u001b[1;34m(self, width, height, dpi)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwidth \u001b[38;5;241m=\u001b[39m width\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheight \u001b[38;5;241m=\u001b[39m height\n\u001b[1;32m---> 70\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_renderer \u001b[38;5;241m=\u001b[39m \u001b[43m_RendererAgg\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mheight\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdpi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_filter_renderers \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_methods()\n",
      "\u001b[1;31mValueError\u001b[0m: Image size of 3974x199649 pixels is too large. It must be less than 2^16 in each direction."
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "samples_per_class = 12\n",
    "samples = []\n",
    "for y, cls in enumerate(classes):\n",
    "    plt.text(-4, 34 * y + 18, cls, ha='right')\n",
    "    idxs, = (y_train == y).nonzero(as_tuple=True)\n",
    "    for i in range(samples_per_class):\n",
    "        idx = idxs[random.randrange(idxs.shape[0])].item()\n",
    "        samples.append(x_train[idx])\n",
    "img = torchvision.utils.make_grid(samples, nrow=samples_per_class)\n",
    "plt.imshow(eecs598.tensor_to_image(img))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-nLyYUhBgDKp"
   },
   "source": [
    "## Subsample the dataset\n",
    "When implementing machine learning algorithms, it's usually a good idea to use a small sample of the full dataset. This way your code will run much faster, allowing for more interactive and efficient development. Once you are satisfied that you have correctly implemented the algorithm, you can then rerun with the entire dataset.\n",
    "\n",
    "The function `eecs598.data.cifar10()` can automatically subsample the CIFAR10 dataset for us. To see how to use it, we can check the documentation using the built-in `help` command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "colab_type": "code",
    "id": "K5CYSO_ugyno",
    "outputId": "2d04f92c-92b8-4aa0-b1a8-f29765748353"
   },
   "outputs": [],
   "source": [
    "help(eecs598.data.cifar10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DtBIn0xjhPMd"
   },
   "source": [
    "We will subsample the data to use only 500 training examples and 250 test examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "FFmXwZbnG9ki",
    "outputId": "dc47bcd0-d46b-41d5-bc75-052ff7043d39"
   },
   "outputs": [],
   "source": [
    "num_train = 500\n",
    "num_test = 250\n",
    "\n",
    "x_train, y_train, x_test, y_test = eecs598.data.cifar10(num_train, num_test)\n",
    "\n",
    "print('Training set:', )\n",
    "print('  data shape:', x_train.shape)\n",
    "print('  labels shape: ', y_train.shape)\n",
    "print('Test set:')\n",
    "print('  data shape: ', x_test.shape)\n",
    "print('  labels shape', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-M0pmnWwgFu5"
   },
   "source": [
    "# K-Nearest Neighbors (k-NN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NOZTkdiSmUFc"
   },
   "source": [
    "## Compute distances: Naive implementation\n",
    "Now that we have examined and prepared our data, it is time to implement the kNN classifier. We can break the process down into two steps:\n",
    "\n",
    "1. Compute the (squared Euclidean) distances between all training examples and all test examples\n",
    "2. Given these distances, for each test example find its k nearest neighbors and have them vote for the label to output\n",
    "\n",
    "Lets begin with computing the distance matrix between all training and test examples. First we will implement a naive version of the distance computation, using explicit loops over the training and test sets. In the file `knn.py`, implement the function `compute_distances_two_loops`.\n",
    "\n",
    "**NOTE: When implementing distance functions for this assignment, you may not use functions `torch.norm` or `torch.dist` (or their instance method variants `x.norm` / `x.dist`); you may not use any functions from `torch.nn` or `torch.nn.functional`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "oHq2bs_MnqVM",
    "outputId": "29d3c420-e982-4a1b-f9c8-61dfe4bbe7b9"
   },
   "outputs": [],
   "source": [
    "from knn import compute_distances_two_loops\n",
    "\n",
    "torch.manual_seed(0)\n",
    "num_train = 500\n",
    "num_test = 250\n",
    "x_train, y_train, x_test, y_test = eecs598.data.cifar10(num_train, num_test)\n",
    "\n",
    "dists = compute_distances_two_loops(x_train, x_test)\n",
    "print('dists has shape: ', dists.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MGdFIqBEpPcQ"
   },
   "source": [
    "As a visual debugging step, we can visualize the distance matrix, where each row is a test example and each column is a training example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 512
    },
    "colab_type": "code",
    "id": "dshO3kmOKk0T",
    "outputId": "4ef94d74-8700-4dd8-f9f4-936fcf0e5125"
   },
   "outputs": [],
   "source": [
    "plt.imshow(dists.numpy(), cmap='gray', interpolation='none')\n",
    "plt.colorbar()\n",
    "plt.xlabel('test')\n",
    "plt.ylabel('train')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aHkuvdr_1HqC"
   },
   "source": [
    "## Compute distances: Vectorization\n",
    "Our implementation of the distance computation above is fairly inefficient since it uses nested Python loops over the training and test sets.\n",
    "\n",
    "When implementing algorithms in PyTorch, it's best to avoid loops in Python if possible. Instead it is preferable to implement your computation so that all loops happen inside PyTorch functions. This will usually be much faster than writing your own loops in Python, since PyTorch functions can be internally optimized to iterate efficiently, possibly using multiple threads. This is especially important when using a GPU to accelerate your code.\n",
    "\n",
    "The process of eliminating explict loops from your code is called **vectorization**. Sometimes it is straighforward to vectorize code originally written with loops; other times vectorizing requires thinking about the problem in a new way. We will use vectorization to improve the speed of our distance computation function.\n",
    "\n",
    "As a first step toward vectorizing our distance computation, you will implement a version that uses only a single Python loop over the training data. In the file `knn.py`, complete the implementation of the function `compute_distances_one_loop`.\n",
    "\n",
    "We can check the correctness of our one-loop implementation by comparing it with our two-loop implementation on some randomly generated data.\n",
    "\n",
    "Note that we do the comparison with 64-bit floating points for increased numeric precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "ujU8bWch4EmK",
    "outputId": "d322edba-53d8-47a5-cd90-069ba57e0b12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference:  0.0\n",
      "Good! The distance matrices match\n"
     ]
    }
   ],
   "source": [
    "from knn import compute_distances_one_loop\n",
    "from knn import compute_distances_two_loops\n",
    "\n",
    "torch.manual_seed(0)\n",
    "x_train_rand = torch.randn(100, 3, 16, 16, dtype=torch.float64)\n",
    "x_test_rand = torch.randn(100, 3, 16, 16, dtype=torch.float64)\n",
    "\n",
    "dists_one = compute_distances_one_loop(x_train_rand, x_test_rand)\n",
    "dists_two = compute_distances_two_loops(x_train_rand, x_test_rand)\n",
    "difference = (dists_one - dists_two).pow(2).sum().sqrt().item()\n",
    "print('Difference: ', difference)\n",
    "if difference < 1e-4:\n",
    "    print('Good! The distance matrices match')\n",
    "else:\n",
    "    print('Uh-oh! The distance matrices are different')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gqtIsY6x_kb9"
   },
   "source": [
    "You will now implement a fully vectorized version of the distance computation function\n",
    "that does not use any Python loops. In the file `knn.py`, implement the function `compute_distances_no_loops`.\n",
    "\n",
    "As before, we can check the correctness of our implementation by comparing the fully vectorized version against the original naive version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "1RY8QBeS9WYK",
    "outputId": "b8ed1d8e-cd2f-4a84-864a-08dbd51c1698"
   },
   "outputs": [],
   "source": [
    "from knn import compute_distances_two_loops\n",
    "from knn import compute_distances_no_loops\n",
    "\n",
    "torch.manual_seed(0)\n",
    "x_train_rand = torch.randn(100, 3, 16, 16, dtype=torch.float64)\n",
    "x_test_rand = torch.randn(100, 3, 16, 16, dtype=torch.float64)\n",
    "\n",
    "dists_two = compute_distances_two_loops(x_train_rand, x_test_rand)\n",
    "dists_none = compute_distances_no_loops(x_train_rand, x_test_rand)\n",
    "difference = (dists_two - dists_none).pow(2).sum().sqrt().item()\n",
    "print('Difference: ', difference)\n",
    "if difference < 1e-4:\n",
    "  print('Good! The distance matrices match')\n",
    "else:\n",
    "  print('Uh-oh! The distance matrices are different')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0JPMM0-BBGmt"
   },
   "source": [
    "We can now compare the speed of our three implementations. If you've implemented everything properly, the one-loop implementation should take less than 4 seconds to run, and the fully vectorized implementation should take less than 0.1 seconds to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "IN9cntDC5c5q",
    "outputId": "b78643e0-ce71-41b1-ffed-6f3e625ceafe"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from knn import compute_distances_two_loops\n",
    "from knn import compute_distances_one_loop\n",
    "from knn import compute_distances_no_loops\n",
    "\n",
    "def timeit(f, *args):\n",
    "    tic = time.time()\n",
    "    f(*args) \n",
    "    toc = time.time()\n",
    "    return toc - tic\n",
    "\n",
    "torch.manual_seed(0)\n",
    "x_train_rand = torch.randn(500, 3, 32, 32)\n",
    "x_test_rand = torch.randn(500, 3, 32, 32)\n",
    "\n",
    "two_loop_time = timeit(compute_distances_two_loops, x_train_rand, x_test_rand)\n",
    "print('Two loop version took %.2f seconds' % two_loop_time)\n",
    "\n",
    "one_loop_time = timeit(compute_distances_one_loop, x_train_rand, x_test_rand)\n",
    "speedup = two_loop_time / one_loop_time\n",
    "print('One loop version took %.2f seconds (%.1fX speedup)'\n",
    "      % (one_loop_time, speedup))\n",
    "\n",
    "no_loop_time = timeit(compute_distances_no_loops, x_train_rand, x_test_rand)\n",
    "speedup = two_loop_time / no_loop_time\n",
    "print('No loop version took %.2f seconds (%.1fX speedup)'\n",
    "      % (no_loop_time, speedup))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EudsSj5TrGGF"
   },
   "source": [
    "## Predict labels\n",
    "Now that we have a method for computing distances between training and test examples, we need to implement a function that uses those distances together with the training labels to predict labels for test samples.\n",
    "\n",
    "In the file `knn.py`, implement the function `predict_labels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "MWk4BTMKfWz8",
    "outputId": "e4aa005a-ab93-4232-b27c-c1c365246f7d"
   },
   "outputs": [],
   "source": [
    "from knn import predict_labels\n",
    "\n",
    "torch.manual_seed(0)\n",
    "dists = torch.tensor([\n",
    "    [0.3, 0.4, 0.1],\n",
    "    [0.1, 0.5, 0.5],\n",
    "    [0.4, 0.1, 0.2],\n",
    "    [0.2, 0.2, 0.4],\n",
    "    [0.5, 0.3, 0.3],\n",
    "])\n",
    "y_train = torch.tensor([0, 1, 0, 1, 2])\n",
    "y_pred_expected = torch.tensor([1, 0, 0])\n",
    "y_pred = predict_labels(dists, y_train, k=3)\n",
    "correct = y_pred.tolist() == y_pred_expected.tolist()\n",
    "print('Correct: ', correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fMBf1Z6VF9hx"
   },
   "source": [
    "Now we have implemented all the required functionality for the K-Nearest Neighbor classifier. In the file `knn.py`, complete the implementation of the `KnnClassifer` class.\n",
    "\n",
    "We can get some intuition into the KNN classifier by visualizing its predictions on toy 2D data. Here we will generate some random training and test points in 2D, and assign random labels to the training points. We can then make predictions for the test points, and visualize both training and test points. Training points are shown as stars, and tet points are shown as small transparent circles. The color of each point denots its label -- ground-truth label for training points, and predicted label for test points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "zTa7xowOfWz-",
    "outputId": "742be2a6-8d9c-47f2-8773-84f558b4f42d"
   },
   "outputs": [],
   "source": [
    "from knn import KnnClassifier\n",
    "\n",
    "num_test = 10000\n",
    "num_train = 20\n",
    "num_classes = 5\n",
    "\n",
    "# Generate random training and test data\n",
    "torch.manual_seed(128)\n",
    "x_train = torch.rand(num_train, 2)\n",
    "y_train = torch.randint(num_classes, size=(num_train,))\n",
    "x_test = torch.rand(num_test, 2)\n",
    "classifier = KnnClassifier(x_train, y_train)\n",
    "\n",
    "# Plot predictions for different values of k\n",
    "for k in [1, 3, 5]:\n",
    "    y_test = classifier.predict(x_test, k=k)\n",
    "    plt.gcf().set_size_inches(8, 8)\n",
    "    class_colors = ['r', 'g', 'b', 'k', 'y']\n",
    "    train_colors = [class_colors[c] for c in y_train]\n",
    "    test_colors = [class_colors[c] for c in y_test]\n",
    "    plt.scatter(x_test[:, 0], x_test[:, 1],\n",
    "                color=test_colors, marker='o', s=32, alpha=0.05)\n",
    "    plt.scatter(x_train[:, 0], x_train[:, 1],\n",
    "                color=train_colors, marker='*', s=128.0)\n",
    "    plt.title('Predictions for k = %d' % k, size=16)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2tgNeDX0fW0A"
   },
   "source": [
    "We can use the exact same KNN code to perform image classification on CIFAR-10!\n",
    "\n",
    "Now lets put everything together and test our K-NN clasifier on a subset of CIFAR-10, using k=1:\n",
    "\n",
    "If you've implemented everything correctly you should see an accuracy of about 27%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "W5GVNBh0ySGN",
    "outputId": "32a4ed21-8d39-4c03-ed72-59a6e963da8e"
   },
   "outputs": [],
   "source": [
    "from knn import KnnClassifier\n",
    "\n",
    "torch.manual_seed(0)\n",
    "num_train = 5000\n",
    "num_test = 500\n",
    "x_train, y_train, x_test, y_test = eecs598.data.cifar10(num_train, num_test)\n",
    "\n",
    "classifier = KnnClassifier(x_train, y_train)\n",
    "classifier.check_accuracy(x_test, y_test, k=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QQwHpcPrIF5u"
   },
   "source": [
    "Now lets increase to k=5. You should see a slightly higher accuracy than k=1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "_a4zwcTe0PIK",
    "outputId": "f1d12b4a-8a70-4b8c-a6d4-8565036e57dd"
   },
   "outputs": [],
   "source": [
    "from knn import KnnClassifier\n",
    "\n",
    "torch.manual_seed(0)\n",
    "num_train = 5000\n",
    "num_test = 500\n",
    "x_train, y_train, x_test, y_test = eecs598.data.cifar10(num_train, num_test)\n",
    "\n",
    "classifier = KnnClassifier(x_train, y_train)\n",
    "classifier.check_accuracy(x_test, y_test, k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QNyZLRmaIgT0"
   },
   "source": [
    "## Cross-validation\n",
    "We have not implemented the full k-Nearest Neighbor classifier, but the choice of $k=5$ was arbitrary. We will use **cross-validation** to set this hyperparameter in a more principled manner.\n",
    "\n",
    "In the file `knn.py`, implement the function `knn_cross_validate` to perform cross-validation on k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "pA5MrumnLk5B",
    "outputId": "be8c8630-d020-4f79-f336-b76ce1f4bf6c"
   },
   "outputs": [],
   "source": [
    "from knn import knn_cross_validate\n",
    "\n",
    "torch.manual_seed(0)\n",
    "num_train = 5000\n",
    "num_test = 500\n",
    "x_train, y_train, x_test, y_test = eecs598.data.cifar10(num_train, num_test)\n",
    "\n",
    "k_to_accuracies = knn_cross_validate(x_train, y_train, num_folds=5)\n",
    "\n",
    "for k, accs in sorted(k_to_accuracies.items()):\n",
    "  print('k = %d got accuracies: %r' % (k, accs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 528
    },
    "colab_type": "code",
    "id": "vMtPikIsNxl2",
    "outputId": "0443bc91-e5ec-4fb9-e430-62806604609e"
   },
   "outputs": [],
   "source": [
    "ks, means, stds = [], [], []\n",
    "torch.manual_seed(0)\n",
    "for k, accs in sorted(k_to_accuracies.items()):\n",
    "  plt.scatter([k] * len(accs), accs, color='g')\n",
    "  ks.append(k)\n",
    "  means.append(statistics.mean(accs))\n",
    "  stds.append(statistics.stdev(accs))\n",
    "plt.errorbar(ks, means, yerr=stds)\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Cross-validation accuracy')\n",
    "plt.title('Cross-validation on k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XZ3Ue0bxmObU"
   },
   "source": [
    "Now we can use the results of cross-validation to select the best value for k, and rerun the classifier on our full 5000 set of training examples.\n",
    "\n",
    "You should get an accuracy above 28%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "NBZfp1UtWyoG",
    "outputId": "b914f55f-070b-46b5-8bca-f84d7e82d88f"
   },
   "outputs": [],
   "source": [
    "from knn import KnnClassifier\n",
    "from knn import knn_get_best_k\n",
    "\n",
    "best_k = 1\n",
    "torch.manual_seed(0)\n",
    "\n",
    "best_k = knn_get_best_k(k_to_accuracies)    \n",
    "print('Best k is ', best_k)\n",
    "\n",
    "classifier = KnnClassifier(x_train, y_train)\n",
    "classifier.check_accuracy(x_test, y_test, k=best_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R1LevOE5mYJh"
   },
   "source": [
    "Finally, we can use our chosen value of k to run on the entire training and test sets.\n",
    "\n",
    "This may take a while to run, since the full training and test sets have 50k and 10k examples respectively. You should get an accuracy above 33%.\n",
    "\n",
    "**Run this only once!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "5gcXjsjFkcGV",
    "outputId": "a5d21480-6f1d-456c-8b54-d56ba26f0550"
   },
   "outputs": [],
   "source": [
    "from knn import KnnClassifier\n",
    "\n",
    "torch.manual_seed(0)\n",
    "x_train_all, y_train_all, x_test_all, y_test_all = eecs598.data.cifar10()\n",
    "classifier = KnnClassifier(x_train_all, y_train_all)\n",
    "classifier.check_accuracy(x_test_all, y_test_all, k=best_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_eeuN70qb1oy"
   },
   "source": [
    "## Submit Your Work\n",
    "After completing both notebooks for this assignment (`pytorch101.ipynb` and this notebook, `knn.ipynb`), run the following cell to create a `.zip` file for you to download and turn in. **Please MANUALLY SAVE every `*.ipynb` and `*.py` files before executing the following cell:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "3kXg-9z8b1oz",
    "outputId": "072fb23b-b4ab-4942-ae58-1c89329d4829"
   },
   "outputs": [],
   "source": [
    "from eecs598.submit import make_a1_submission\n",
    "\n",
    "make_a1_submission(GOOGLE_DRIVE_PATH)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "hQrEwOpXb9Gh",
    "Cnf0BfHZfWzO",
    "SWSgBT8Wf3tW",
    "emQnvtnFeX1H",
    "GSd6jQb4epkC",
    "AKKdLGIIffYx",
    "-nLyYUhBgDKp",
    "NOZTkdiSmUFc",
    "aHkuvdr_1HqC",
    "EudsSj5TrGGF",
    "QNyZLRmaIgT0"
   ],
   "name": "knn.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
