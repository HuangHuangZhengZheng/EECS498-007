{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "639103b1bc2965c",
   "metadata": {},
   "source": [
    "## Tensor基础"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489d5a7f8198704f",
   "metadata": {},
   "source": [
    "### 2.1 Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T11:22:08.911606Z",
     "start_time": "2024-09-08T11:22:08.907526Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91abdcaa04dc151b",
   "metadata": {},
   "source": [
    "#### 基本构造方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45179355d27da058",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T11:22:08.876814800Z",
     "start_time": "2024-09-08T08:38:24.664439Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-8.1173e+31,  2.0361e-42,  0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.Tensor(2,3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "693fc476aaae44de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T11:22:08.895643900Z",
     "start_time": "2024-09-08T08:38:26.281362Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 1, 0],\n",
       "        [0, 0, 1]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list = [[0, 0, 0], [1, 0, 0], [0, 1, 0], [0, 0, 1]]\n",
    "fromList = torch.tensor(list)\n",
    "fromList"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5471af719b913756",
   "metadata": {},
   "source": [
    "#### 快速创建方法\n",
    "- 全0 or 全1\n",
    "- 对角1\n",
    "- rand: 随机[0, 1)\n",
    "- arange: 区间 + optional 步长"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bbcc80fa306fcf1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T11:22:08.897640500Z",
     "start_time": "2024-09-08T08:38:26.310236Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.]])\n",
      "tensor([1.0000, 1.5000, 2.0000, 2.5000, 3.0000, 3.5000])\n"
     ]
    }
   ],
   "source": [
    "print(torch.zeros(2,3,2))\n",
    "print(torch.ones(2,3))\n",
    "print(torch.eye(2,3))\n",
    "print(torch.arange(1, 4, 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f566369619475e3",
   "metadata": {},
   "source": [
    "#### 常见属性与操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51d48ea81ba9fdeb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T11:22:08.897640500Z",
     "start_time": "2024-09-08T08:38:26.552203Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.FloatTensor\n",
      "torch.float32\n",
      "fromList shape is torch.Size([4, 3])\n",
      "tensor(0)\n"
     ]
    }
   ],
   "source": [
    "print(x.type())\n",
    "print(x.dtype)\n",
    "print('fromList shape is ' + str(fromList.shape))\n",
    "print(fromList[0][0]) # a tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b819d75f5b83832",
   "metadata": {},
   "source": [
    "#### 常见数学操作\n",
    "**static and non-static method can both apply in some case** :grinning:\n",
    "\n",
    "如果操作带*下划线*, 返回值将会覆盖对象 :thinking:\n",
    "\n",
    "- `add()`, `mul()`, `div()`: 同操作一个标量, 或者Tensor之间逐个元素操作\n",
    "- `fmod()`, `remainder()`: 取余数\n",
    "- `ceil()`, `floor()`, `log()`, `exp()`, `sigmoid()`, `sqrt()`, `abs()`, `min()`, `max()`: 懂得都懂\n",
    "- `clamp()`: 至少传入一个标量, 取上下限\n",
    "- `round()`: 取最近的整数\n",
    "- `frac()`: 取小数部分\n",
    "- `neg()`, `reciprocal()`: 取负, 取倒数\n",
    "- `pow()`: 取幂, 必要时LLM\n",
    "- `sign()`: +-1显示正负\n",
    "- `dist()`：返回两个Tensor之间的距离(范数)\n",
    "- `norm()`: 计算Tensor范数\n",
    "- `sum()`, `prod()`: 返回所有元素之和/积\n",
    "- `torch.mv()`, `torch.mm()`: matrix/vector乘法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d81acba4ed2698c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T11:22:08.897640500Z",
     "start_time": "2024-09-08T08:45:49.837903Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.,  1., -1.,  1.])\n",
      "tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "l = [-1, +1, -0.255, +6.12]\n",
    "l1 = torch.tensor(l)\n",
    "print(l1.sign())\n",
    "print(l1.dist(l1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e268921f",
   "metadata": {},
   "source": [
    "- 爱因斯坦求和约定 `einsum()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501676d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "z_norm = torch.randn(2, 3, 4)\n",
    "\n",
    "# 使用 torch.einsum 计算相似性矩阵\n",
    "z_cos_sim = torch.einsum('bci,bcj->bij', z_norm, z_norm)\n",
    "\n",
    "print(z_cos_sim.shape)  # 输出：torch.Size([2, 4, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02e3463",
   "metadata": {},
   "source": [
    "`torch.einsum` 是 PyTorch 中基于 **爱因斯坦求和约定**（Einstein Summation Convention）实现的函数，用于高效、简洁地表达多维张量的复杂运算。它能够替代矩阵乘法、转置、求和、点积、外积等多种操作，尤其适合处理高维张量的操作。\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "爱因斯坦求和约定通过省略求和符号（`Σ`），用下标标记张量的维度，直接表达乘积和求和的规则。例如：\n",
    "- 矩阵乘法 `C = A @ B` 可表示为 `ik,kj->ij`，含义是对中间维度 `k` 求和。\n",
    "- 向量内积 `a·b` 可表示为 `i,i->`，即对相同下标 `i` 的乘积求和。\n",
    "\n",
    "---\n",
    "\n",
    "### **`torch.einsum` 语法**\n",
    "```python\n",
    "torch.einsum(equation, *operands)\n",
    "```\n",
    "- **`equation`**: 字符串，描述操作的维度规则（如 `\"ik,kj->ij\"`）。\n",
    "- **`operands`**: 输入张量，数量与方程中的输入数量一致。\n",
    "\n",
    "---\n",
    "\n",
    "### **关键规则**\n",
    "1. **下标字符**：用字母（如 `i, j, k`）标记维度。\n",
    "2. **逗号分隔输入**：输入张量的维度用逗号分隔（如 `\"ik,kj\"` 表示两个输入）。\n",
    "3. **箭头后为输出**：`->` 后的字符表示输出维度。\n",
    "4. **隐式求和**：在方程中出现但未在输出中标记的下标会被求和。\n",
    "\n",
    "---\n",
    "\n",
    "### **常见用例与示例**\n",
    "\n",
    "#### 1. **矩阵乘法**\n",
    "```python\n",
    "A = torch.randn(2, 3)\n",
    "B = torch.randn(3, 4)\n",
    "C = torch.einsum(\"ik,kj->ij\", A, B)  # 等价于 A @ B\n",
    "```\n",
    "\n",
    "#### 2. **逐元素乘法**\n",
    "```python\n",
    "A = torch.randn(2, 3)\n",
    "B = torch.randn(2, 3)\n",
    "C = torch.einsum(\"ij,ij->ij\", A, B)  # 等价于 A * B\n",
    "```\n",
    "\n",
    "#### 3. **求和**\n",
    "```python\n",
    "A = torch.randn(2, 3)\n",
    "sum_all = torch.einsum(\"ij->\", A)      # 所有元素求和，等价于 A.sum()\n",
    "sum_row = torch.einsum(\"ij->i\", A)    # 按行求和，等价于 A.sum(dim=1)\n",
    "```\n",
    "\n",
    "#### 4. **转置**\n",
    "```python\n",
    "A = torch.randn(2, 3)\n",
    "A_T = torch.einsum(\"ij->ji\", A)  # 等价于 A.T 或 A.permute(1,0)\n",
    "```\n",
    "\n",
    "#### 5. **对角线元素**\n",
    "```python\n",
    "A = torch.randn(3, 3)\n",
    "diag = torch.einsum(\"ii->i\", A)  # 提取对角线元素，等价于 A.diag()\n",
    "```\n",
    "\n",
    "#### 6. **批量矩阵乘法**\n",
    "```python\n",
    "A = torch.randn(5, 2, 3)  # batch=5\n",
    "B = torch.randn(5, 3, 4)\n",
    "C = torch.einsum(\"bij,bjk->bik\", A, B)  # 等价于 A @ B\n",
    "```\n",
    "\n",
    "#### 7. **外积**\n",
    "```python\n",
    "a = torch.randn(3)\n",
    "b = torch.randn(4)\n",
    "outer = torch.einsum(\"i,j->ij\", a, b)  # 外积，等价于 torch.outer(a, b)\n",
    "```\n",
    "\n",
    "#### 8. **张量缩并（Tensor Contraction）**\n",
    "```python\n",
    "A = torch.randn(3, 4, 5)\n",
    "B = torch.randn(5, 3, 6)\n",
    "C = torch.einsum(\"ijk,kil->jl\", A, B)  # 缩并维度 k 和 i\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **广播机制**\n",
    "`einsum` 支持自动广播。例如，向量与矩阵的乘法：\n",
    "```python\n",
    "a = torch.randn(3)\n",
    "B = torch.randn(5, 3, 4)\n",
    "C = torch.einsum(\"i, b i j -> b j\", a, B)  # 结果形状 (5, 4)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **注意事项**\n",
    "1. **效率**：对于简单操作（如矩阵乘法），直接使用内置函数（如 `torch.matmul`）可能更高效。\n",
    "2. **维度匹配**：输入张量的对应维度大小必须一致，否则报错。\n",
    "3. **可读性**：复杂的方程可能降低代码可读性，建议添加注释。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a242e02b60309277",
   "metadata": {},
   "source": [
    "#### 常见线性代数数学操作\n",
    "kind of like non-static in torch\n",
    "- `mv()`, `mm()`: matrix/vector乘法\n",
    "- `dot()`: 点乘\n",
    "- `addmm()`, `addmv()`, `addr()`: 两个Tensor操作后与其中一个相加\n",
    "- `bmm()`, `addbmm()`, `baddbmm()`: 不常见, 必要时LLM\n",
    "- `eig()`: 求特征值和特征向量\n",
    "- `ger()`: 求两个向量的张量积\n",
    "- `inverse()`: 求逆\n",
    "- `t()`: 转置, 仅限二维矩阵"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf56af8b325cfb4",
   "metadata": {},
   "source": [
    "#### 连接和切片\n",
    "torch non-static\n",
    "- `cat()`: 第一个参数将所有需要连接的Tensor组成一个元组传入, 第二个参数是连接维度\n",
    "- `chunk()`: 第一被切的对象, 第二切分块数, 第三切分维度\n",
    "- `index_select()`: 类比numpy dataframe的切片\n",
    "- `unbind()`: 沿着给定维度进行逐个单位拆分, 返回序列\n",
    "- `split()`: 切成相等形状的块\n",
    "- `nonzero()`: 返回非零索引的Tensor\n",
    "- `squeeze()`: 除掉维数1\n",
    "- `unsqueeze()`: 指定维度添加1\n",
    "- `stack()`: 和`cat()`不同在于会添加维度\n",
    "- `transpose()`: 对Tensor指定的两个维度进行转置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37b7b6445c3c8384",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T11:24:41.511389Z",
     "start_time": "2024-09-08T11:24:41.503427Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.3588, -1.2730,  1.0648,  0.7868],\n",
      "        [-1.4120, -0.7000, -0.5600,  0.8299],\n",
      "        [ 0.0598,  0.4924, -0.7204, -1.2977]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3588, -1.2730,  1.0648,  0.7868],\n",
       "        [ 0.0598,  0.4924, -0.7204, -1.2977]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(3, 4)\n",
    "print(x)\n",
    "indices = torch.tensor([0, 2])\n",
    "selected_rows = torch.index_select(x, 0, indices)\n",
    "selected_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "17b6a9714bea1c46",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T11:31:22.726678Z",
     "start_time": "2024-09-08T11:31:22.720862Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 0],\n",
      "        [2, 0],\n",
      "        [2, 1]])\n"
     ]
    }
   ],
   "source": [
    "c = torch.tensor([[1, 2], [3, 0], [4, 5]])\n",
    "nonzero_and_greater_than_2 = torch.nonzero(c > 2)\n",
    "print(nonzero_and_greater_than_2)  # 输出: tensor([[1, 0],\n",
    "                                  #              [2, 0],\n",
    "                                  #              [2, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a116eef7e1d4577",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T11:33:41.474888Z",
     "start_time": "2024-09-08T11:33:41.465379Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3])\n",
      "tensor([[1],\n",
      "        [2],\n",
      "        [3]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[[1], [2], [3]]])  # 形状为 (1, 3, 1)\n",
    "y = torch.squeeze(x)  # 移除所有大小为 1 的维度，形状变为 (3,)\n",
    "print(y)\n",
    "z = torch.squeeze(x, dim=0) # 指定移除第一个维度，形状变为 (3, 1)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fcf51d43750735da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T11:36:52.768345Z",
     "start_time": "2024-09-08T11:36:52.759218Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1,  2,  3],\n",
      "         [ 4,  5,  6]],\n",
      "\n",
      "        [[ 7,  8,  9],\n",
      "         [10, 11, 12]]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 假设我们有两个形状为 (2, 3) 的张量\n",
    "x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "y = torch.tensor([[7, 8, 9], [10, 11, 12]])\n",
    "\n",
    "# 沿着一个新的维度堆叠它们，结果的形状将是 (2, 2, 3)\n",
    "z = torch.stack((x, y), dim=0)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6400099229b14e99",
   "metadata": {},
   "source": [
    "#### 变形\n",
    "- `view()`: 变形, 当传入-1时会自动计算维度\n",
    "- `flatten()`: 展开, 侧重于沿着指定维度展开\n",
    "- `einops.rearrange()`：变形，支持优雅的变形操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "95b92011ed0bcc98",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T11:39:55.624352Z",
     "start_time": "2024-09-08T11:39:55.617049Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 12])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(2, 3, 4)\n",
    "y = x.view(2,12)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3bdd95db88b6d8bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T11:41:43.273692Z",
     "start_time": "2024-09-08T11:41:43.262012Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23])\n",
      "tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11],\n",
      "        [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]])\n"
     ]
    }
   ],
   "source": [
    "# 创建一个形状为 (2, 3, 4) 的张量\n",
    "x = torch.arange(24).reshape(2, 3, 4)\n",
    "\n",
    "# 将整个张量展平成一维张量\n",
    "y = torch.flatten(x)\n",
    "print(y)  # 输出: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23])\n",
    "\n",
    "# 展平从第二个维度开始的所有元素\n",
    "z = torch.flatten(x, start_dim=1)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fcaaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange\n",
    "\n",
    "z_flat = rearrange(z, 'b c h w -> b c (h w)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6eb00c4ec1d17f7",
   "metadata": {},
   "source": [
    "#### CUDA加速"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "754381fd98dbc5ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T11:44:17.370186Z",
     "start_time": "2024-09-08T11:44:16.474976Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "x = x.cuda()\n",
    "print(x.device)\n",
    "x = x.to('cpu')\n",
    "print(x.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4106b8c4",
   "metadata": {},
   "source": [
    "##### 内存布局\n",
    "- 配合 `view()` 或 `reshape()`：`view()` 和 `reshape()` 需要张量在内存中是连续的，否则会报错。如果张量经过 `transpose()` 或 `permute()` 等操作后变得不连续，可以调用 `contiguous()` 使其连续。\n",
    "- 某些操作（如矩阵乘法、卷积等）需要张量在内存中连续存储以提高计算效率\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbe37ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 创建一个张量\n",
    "x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "# 对张量进行转置操作，导致内存不连续\n",
    "y = x.t()\n",
    "\n",
    "# 检查是否连续\n",
    "print(y.is_contiguous())  # 输出：False\n",
    "\n",
    "# 使用 contiguous() 使其连续\n",
    "z = y.contiguous(memory_format=torch.contiguous_format) # 行优先\n",
    "\n",
    "# 检查是否连续\n",
    "print(z.is_contiguous())  # 输出：True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcdc83cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PRPsu24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
